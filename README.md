# README

This repo hold codes for case study of existing open-sourced diffusion models' capability in text-to-image generation and image editing.

## Model List

### Open-Source Text-to-Image Models:

- [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)
- [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
- [zai-org/CogView3-Plus-3B](https://huggingface.co/zai-org/CogView3-Plus-3B)
- [PixArt-alpha/PixArt-XL-2-512x512](https://huggingface.co/PixArt-alpha/PixArt-XL-2-512x512)
- [PixArt-alpha/PixArt-Sigma-XL-2-512-MS](https://huggingface.co/PixArt-alpha/PixArt-Sigma-XL-2-512-MS)
- [Alpha-VLLM/Lumina-Next-SFT-diffusers](https://huggingface.co/Alpha-VLLM/Lumina-Next-SFT-diffusers)
- [Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers)
- [stabilityai/stable-diffusion-3-medium-diffusers](https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers)
- [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)
- [Efficient-Large-Model/Sana_600M_512px_diffusers](https://huggingface.co/Efficient-Large-Model/Sana_600M_512px_diffusers)
- [Qwen/Qwen-Image](https://huggingface.co/Qwen/Qwen-Image)
- [thu-ml/unidiffuser-v1](https://huggingface.co/thu-ml/unidiffuser-v1)
- [stabilityai/stable-cascade](https://huggingface.co/stabilityai/stable-cascade)
- [zai-org/CogView4-6B](https://huggingface.co/zai-org/CogView4-6B)

### Closed-Source API Services:

- **OpenAI DALL-E**: DALL-E 2 & DALL-E 3 (with quality and style controls)
- **Google Imagen**: Vertex AI Imagen (high-quality photorealistic generation)
- **Bytedance Cloud**: Volcano Engine text-to-image API
- **Kling AI**: Kling image generation models

## Quick Start

### Installation

1. Clone the repository:

```bash
git clone https://github.com/Bili-Sakura/image-generation-case-study.git
cd image-generation-case-study
```

2. Install dependencies:

```bash
# Install base dependencies for open-source models
pip install -r requirements.txt

# Optional: Install API dependencies for closed-source models
pip install -r requirements_api.txt
```

3. (Optional) Configure API keys for closed-source models:

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env and add your API keys
nano .env  # or use your preferred editor
```

Then set the environment variables:

```bash
export OPENAI_API_KEY="your_openai_api_key"
export GOOGLE_API_KEY="your_google_api_key"
export GOOGLE_PROJECT_ID="your_google_project_id"
export BYTEDANCE_API_KEY="your_bytedance_api_key"
export KLING_API_KEY="your_kling_api_key"
```

### Usage

#### Option 1: Gradio Web UI (Recommended)

Launch the interactive web interface:

```bash
python run.py
```

Or run directly:

```bash
python -m src.app
```

This will:

- Open a web browser at `http://localhost:7860`
- Pre-load default models (SD 2.1, SDXL, Stable Cascade, SD3)
- Provide an intuitive UI for text-to-image generation

**Features:**

- ğŸ¨ Multi-model comparison: Generate with multiple models simultaneously
- ğŸ”“ Open-Source Models: 14 local diffusion models
- ğŸ”’ Closed-Source APIs: OpenAI, Google, Bytedance, Kling
- ğŸ”„ API Comparison: Batch generation across multiple API providers
- âš™ï¸ Configurable parameters: Steps, guidance, size, seed
- ğŸ–¼ï¸ Gallery view: See all results with model labels
- ğŸ’¾ Auto-save: Images saved to `outputs/{model_name}/`
- ğŸ“Š Memory efficient: Sequential generation
- ğŸš€ Multi-GPU support: Automatic device mapping for utilizing multiple GPUs

**Developer Mode:**

```bash
python run.py --dev
```

Allows manual model selection and one-time batch loading (port 7861).

#### Option 2: Python API

Use the modules programmatically:

```python
from diffusers import DiffusionPipeline

# Load a pre-trained diffusion pipeline (e.g., Stable Diffusion 2-1)
pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1")

# Generate an image from a text prompt
prompt = "A fantasy landscape with mountains and rivers"
image = pipe(prompt).images[0]

# Save the generated image
image.save("generated_image.png")
```

Or use the built-in model manager:

```python
from src.model_manager import get_model_manager
from src.inference import generate_image

# Load model
manager = get_model_manager()
manager.load_model("stabilityai/stable-diffusion-2-1")

# Generate
image, filepath, seed = generate_image(
    model_id="stabilityai/stable-diffusion-2-1",
    prompt="A fantasy landscape with mountains and rivers",
    num_inference_steps=50,
    guidance_scale=7.5,
    seed=42
)
```

#### Option 3: Closed-Source API Usage

Use the API clients programmatically:

```python
from src.api_clients import get_api_client

# OpenAI DALL-E
client = get_api_client("openai")
image, error = client.generate(
    prompt="A serene Japanese garden with a red bridge",
    width=1024,
    height=1024,
    model="dall-e-3",
    quality="hd",
    style="vivid"
)

if image:
    image.save("output.png")
else:
    print(f"Error: {error}")
```

See `example_api_generate.py` for more examples including batch comparisons across multiple APIs.

## Project Structure

```
image-generation-case-study/
â”œâ”€â”€ src/                          # Application source code
â”‚   â”œâ”€â”€ app.py                    # Main Gradio UI (user mode)
â”‚   â”œâ”€â”€ app_dev.py                # Developer mode UI
â”‚   â”œâ”€â”€ config.py                 # Model configurations
â”‚   â”œâ”€â”€ model_manager.py          # Model loading & caching
â”‚   â”œâ”€â”€ inference.py              # Generation logic
â”‚   â”œâ”€â”€ api_clients.py            # Closed-source API clients
â”‚   â”œâ”€â”€ closed_source_widget.py   # Gradio widget for APIs
â”‚   â”œâ”€â”€ utils.py                  # Utility functions
â”‚   â””â”€â”€ README.md                 # Detailed documentation
â”œâ”€â”€ outputs/                      # Generated images (organized by model)
â”œâ”€â”€ libs/                         # Reference libraries
â”‚   â”œâ”€â”€ diffusers/
â”‚   â”œâ”€â”€ transformers/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ requirements.txt              # Python dependencies (base)
â”œâ”€â”€ requirements_api.txt          # API dependencies (optional)
â”œâ”€â”€ .env.example                  # Environment variable template
â”œâ”€â”€ run.py                        # Launcher script
â””â”€â”€ README.md                     # This file
```

## Features

### Supported Models (14 Total)

| Model                | Size   | VRAM   | Resolution | Notes                 |
| -------------------- | ------ | ------ | ---------- | --------------------- |
| Stable Diffusion 2.1 | Base   | ~4 GB  | 768x768    | Classic, reliable     |
| Stable Diffusion XL  | XL     | ~7 GB  | 1024x1024  | Higher quality        |
| CogView3 Plus 3B     | 3B     | ~6 GB  | 1024x1024  | Multilingual          |
| PixArt-XL 2          | XL     | ~4 GB  | 512x512    | Fast generation       |
| PixArt-Sigma XL 2    | XL     | ~4 GB  | 512x512    | Improved PixArt       |
| Lumina-Next SFT      | Large  | ~8 GB  | 1024x1024  | Advanced architecture |
| HunyuanDiT v1.2      | Large  | ~10 GB | 1024x1024  | Chinese + English     |
| Stable Diffusion 3   | Medium | ~9 GB  | 1024x1024  | Latest SD3            |
| FLUX.1 Dev           | XL     | ~16 GB | 1024x1024  | State-of-the-art      |
| Sana 600M            | Small  | ~3 GB  | 512x512    | Lightweight           |
| Qwen Image           | Large  | ~8 GB  | 1024x1024  | Multimodal            |
| UniDiffuser v1       | Base   | ~5 GB  | 512x512    | Unified model         |
| Stable Cascade       | Large  | ~10 GB | 1024x1024  | Multi-stage           |
| CogView4 6B          | 6B     | ~11 GB | 1024x1024  | Latest CogView        |

**Default models** (pre-loaded on startup):

- Stable Diffusion 2.1
- Stable Diffusion XL
- Stable Cascade
- Stable Diffusion 3

### Closed-Source API Models

| Provider         | Models Available     | Max Resolution | Special Features              |
| ---------------- | -------------------- | -------------- | ----------------------------- |
| OpenAI DALL-E    | DALL-E 2, DALL-E 3   | 1792x1792      | Quality & style controls      |
| Google Imagen    | Imagen v5            | 1536x1536      | Photorealistic generation     |
| Bytedance Cloud  | Text2Img v1          | 2048x2048      | Fast API response             |
| Kling AI         | Kling v1, v1 Pro     | 2048x2048      | High-quality generation       |

**API Features:**

- ğŸ”‘ Secure API key management via environment variables
- ğŸ›ï¸ Provider-specific controls (quality, style for DALL-E)
- ğŸ”„ Batch comparison across multiple APIs
- ğŸ’° Pay-per-use model (requires API credits)
- âš¡ No GPU required (cloud-based generation)

### Generation Parameters

- **Inference Steps**: 10-100 (default: 50)
- **Guidance Scale**: 1.0-20.0 (default: 7.5)
- **Image Sizes**: 512px to 1280px (multiple presets)
- **Seed Control**: Fixed or random (-1)
- **Negative Prompts**: Supported on compatible models

### Multi-GPU Support

The application automatically detects and utilizes multiple GPUs when available:

- **Automatic Device Mapping**: When 2+ GPUs are detected, models are automatically distributed across GPUs using `device_map="auto"`
- **Transparent Operation**: No configuration needed - just run the application
- **GPU Information**: The UI displays all available GPUs and their memory
- **Single GPU Fallback**: Automatically falls back to single GPU mode if only one GPU is available

The system will print GPU information on startup:

```
Found 2 GPU(s):
  GPU 0: NVIDIA A100-SXM4-40GB (40.0 GB)
  GPU 1: NVIDIA A100-SXM4-40GB (40.0 GB)
Multi-GPU mode enabled: 2 GPUs detected
```

### Output Organization

Images are automatically organized:

```
outputs/
â”œâ”€â”€ stabilityai-stable-diffusion-2-1/
â”‚   â”œâ”€â”€ 20251009_140530_seed12345.png
â”‚   â””â”€â”€ 20251009_140530_seed12345.txt
â”œâ”€â”€ stabilityai-stable-diffusion-xl-base-1.0/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

Each generation includes:

- PNG image file
- TXT metadata (model, prompt, seed, timestamp)

## Citation

If you find this repository useful, please cite it as:

```bibtex
@misc{bili_sakura_image_generation_case_study,
  author       = {Bili-Sakura},
  title        = {Image Generation Case Study},
  year         = {2025},
  howpublished = {\url{https://github.com/Bili-Sakura/image-generation-case-study}},
  note         = {Accessed: 2025-10-08}
}
```
