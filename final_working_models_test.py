#!/usr/bin/env python3
"""
Final comprehensive test of all working models after all fixes.
"""

import sys
import time
import gc
import torch
from src.model_manager import get_model_manager
from src.compute_profiler import create_profiler


def free_memory():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def test_model(model_id, description):
    """Test a single model."""
    print(f"\n{'='*90}")
    print(f"{description}")
    print(f"Model: {model_id}")
    print(f"{'='*90}")
    
    manager = get_model_manager()
    profiler = create_profiler(enabled=True)
    
    try:
        pipe = manager.load_model(model_id)
        summary = profiler.summarize_macs(pipe, height=512, width=512, steps=30)
        
        trans = summary['UNet per-step (GMACs)']
        text = summary['Text encoder once (GMACs)']
        vae = summary['VAE decode once (GMACs)']
        total = summary['Total 30 steps (GMACs)']
        
        print(f"Results:")
        print(f"  Transformer/UNet: {trans:>12.3f} GMACs/step")
        print(f"  Text Encoders:    {text:>12.3f} GMACs")
        print(f"  VAE Decoder:      {vae:>12.3f} GMACs")
        print(f"  TOTAL (30 steps): {total:>12.3f} GMACs ({total/1000:.1f} TFLOPs)")
        
        return {
            'transformer': trans,
            'text': text,
            'vae': vae,
            'total': total,
            'success': trans > 0
        }
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {'success': False}
    finally:
        try:
            if model_id in manager.loaded_models:
                del manager.loaded_models[model_id]
            if 'pipe' in locals():
                del pipe
        except:
            pass
        free_memory()
        time.sleep(2)


def main():
    """Test all known working models."""
    print(f"\n{'üèÜ'*45}")
    print(f"FINAL COMPREHENSIVE TEST - ALL FIXED MODELS")
    print(f"{'üèÜ'*45}\n")
    
    print("Testing all models that should now work with our fixes...")
    
    models = [
        # Proven working models
        ("stabilityai/stable-diffusion-3-medium-diffusers", "SD3-medium (Reference - Flow Matching)"),
        ("stabilityai/stable-diffusion-2-1-base", "SD2.1-base (Reference - Classic Diffusion)"),
        
        # Newly fixed with GenericTransformerWrapper
        ("Efficient-Large-Model/Sana_600M_512px_diffusers", "SANA-600M (Fixed - Highest Compute)"),
        ("Efficient-Large-Model/SANA1.5_1.6B_1024px_diffusers", "SANA-1.6B (Fixed - Highest Compute)"),
        ("PixArt-alpha/PixArt-XL-2-512x512", "PixArt-XL (Fixed - DiT Architecture)"),
        
        # Newly fixed special wrappers
        ("Alpha-VLLM/Lumina-Image-2.0", "Lumina-Image-2.0 (Fixed - Lumina2Wrapper)"),
        ("kandinsky-community/kandinsky-3", "Kandinsky-3 (Fixed - KandinskyWrapper)"),
    ]
    
    results = []
    for model_id, description in models:
        result = test_model(model_id, description)
        result['model_id'] = model_id
        result['description'] = description
        results.append(result)
    
    # Final Summary
    print(f"\n\n{'='*90}")
    print(f"FINAL SUMMARY - ALL WORKING MODELS")
    print(f"{'='*90}\n")
    
    print(f"{'Model':<40} {'Transformer':<15} {'Text':<12} {'VAE':<12} {'Total (TFLOPs)':<15}")
    print(f"{'-'*90}")
    
    working = [r for r in results if r.get('success', False)]
    for r in working:
        name = r['model_id'].split('/')[-1][:38]
        trans = r.get('transformer', 0)
        text = r.get('text', 0)
        vae = r.get('vae', 0)
        total = r.get('total', 0)
        print(f"{name:<40} {trans:>10.1f} G   {text:>8.1f} G  {vae:>8.1f} G  {total/1000:>10.1f}")
    
    print(f"{'-'*90}")
    print(f"\n‚úÖ TOTAL: {len(working)}/{len(results)} models fully working")
    print(f"{'='*90}\n")
    
    # Key insights
    if working:
        print("üî¨ Key Insights:")
        print(f"   Highest Compute: SANA-1.6B ({max(r.get('total', 0) for r in working)/1000:.1f} TFLOPs)")
        print(f"   Lowest Compute: SD2.1 ({min(r.get('total', 0) for r in working if r.get('total', 0) > 0)/1000:.1f} TFLOPs)")
        
        avg_trans = sum(r.get('transformer', 0) for r in working) / len(working)
        print(f"   Average Transformer: {avg_trans:.1f} GMACs/step")
        
        print(f"\nüéØ Profiler now supports {len(working)} diverse architectures!")
    
    return 0 if len(working) >= 5 else 1


if __name__ == "__main__":
    sys.exit(main())

