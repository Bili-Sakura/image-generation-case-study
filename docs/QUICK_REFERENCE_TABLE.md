# Quick Reference: DiT Architecture Comparison

## ðŸ“Š At-A-Glance Comparison Table

| Model                  | Year    | Params | Architecture      | Text Handling         | Timestep            | Block Type             | âœ… Your Figure Shows |
| ---------------------- | ------- | ------ | ----------------- | --------------------- | ------------------- | ---------------------- | -------------------- |
| **PixArt-Î±/Î£**         | 2023-24 | 5.46B  | **Cross-Attn**    | Separate (cross-attn) | Time only           | Self-Attn + Cross-Attn | âœ“ Correct            |
| **Lumina-T2I/Next**    | 2024    | 4.34B  | **Cross-Attn**    | Separate (cross-attn) | Time only           | Self-Attn + Cross-Attn | âœ“ Correct            |
| **Stable Diffusion 3** | 2024    | 7.69B  | **Double-Stream** | Parallel streams      | **Time + Text**     | Joint Attn (âŠ™)         | âœ“ Correct            |
| **FLUX.1-Dev**         | 2024    | 16.87B | **Hybrid**        | Double + Single       | **Time + Text**     | Mixed (âŠ™)              | âœ“ Correct            |
| **CogView3-Plus**      | 2024    | 8.02B  | **Double-Stream** | Parallel streams      | Time + Size         | Joint Attn             | Similar to SD3       |
| **Hunyuan-DiT**        | 2024    | 3.61B  | **Cross-Attn**    | Separate (cross-attn) | Time + Text + Style | Self-Attn + Cross-Attn | Similar to PixArt    |
| **SANA**               | 2025    | 3.52B  | **Cross-Attn**    | Separate (cross-attn) | Time + Guidance     | Self-Attn + Cross-Attn | Similar to PixArt    |
| **CogView4-6B**        | 2025    | 6.00B  | **Double-Stream** | Concat â†’ Split        | Time + Size         | Concat Joint Attn      | Similar to SD3       |
| **Qwen-Image**         | 2025    | 28.85B | **Double-Stream** | Parallel streams      | Time only           | Joint Attn             | Similar to SD3       |
| **OmniGen**            | N/A     | N/A    | **Single-Stream** | **Token Concat**      | **Time as Token**   | Unified Self-Attn (âŠ™)  | âœ“ Correct            |

---

## ðŸŽ¯ Three Core Architectures

### 1ï¸âƒ£ Cross-Attention Architecture

**Used by**: PixArt, Lumina, Hunyuan-DiT, SANA

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Tokens â”‚â”€â”€â”€â”€â”     â”‚ Noised Image â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚            â”‚
                   â”‚            â†“
                   â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚     â”‚ Self-Attn   â”‚
                   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚            â”‚
                   â”‚            â†“
                   â””â”€â”€â”€â”€â”€â†’ Cross-Attn
                   (K,V)   (Q from img)
                               â”‚
                               â†“
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   FFN   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â†“
                           Output Image
```

**Key**: Text and image stay **SEPARATE**. Image queries attend to text.

---

### 2ï¸âƒ£ Double-Stream Architecture

**Used by**: SD3, FLUX (early), CogView3/4, Qwen, HiDream

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Stream â”‚          â”‚ Image Stream â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â†“                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Norm + adaLNâ”‚          â”‚ Norm + adaLN â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Joint Attn âŠ™ â”‚  â† Both streams process together
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â†“                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text FFN    â”‚          â”‚ Image FFN    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â†“                        â†“
  Text Output              Image Output
```

**Key**: Two parallel streams with **joint attention** mechanism. Both get updated.

---

### 3ï¸âƒ£ Single-Stream Architecture

**Used by**: OmniGen

```
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Text â”‚  â”‚Time  â”‚  â”‚Image  â”‚
â”‚Tokenâ”‚  â”‚Token â”‚  â”‚Tokens â”‚
â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚        â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  CONCATENATE  â”‚ âŠ™
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
    [Tâ‚, Tâ‚‚, ..., Tâ‚™, TIME, Iâ‚, Iâ‚‚, ..., Iâ‚˜]
            â”‚
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Self-Attn    â”‚  â† Single attention over all tokens
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     FFN       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
      Extract Image Tokens â†’ Output
```

**Key**: Everything is a token. **ONE sequence**. Processed together via self-attention.

---

## ðŸ”‘ Key Distinctions

| Feature                 | Cross-Attn           | Double-Stream   | Single-Stream        |
| ----------------------- | -------------------- | --------------- | -------------------- |
| **Modality Separation** | âœ… Complete          | âš¡ Parallel     | âŒ Unified           |
| **Textâ†’Image Flow**     | One-way (cross-attn) | Bidirectional   | Implicit (attention) |
| **Concatenation**       | âŒ NO                | âŒ NO\*         | âœ… YES               |
| **Attention Ops/Block** | 2 (self + cross)     | 1 (joint)       | 1 (self over all)    |
| **Timestep**            | Modulation           | Time+Text fused | Token in sequence    |
| **Complexity**          | Medium               | High            | Low\*\*              |
| **Flexibility**         | Good                 | Better          | Best                 |

\* Except CogView4 variant which concatâ†’attnâ†’split  
\*\* Architecture is simple, but masking can be complex

---

## ðŸ’¡ Understanding Timestep

### Time Only (PixArt, Lumina, Qwen)

```python
timestep â†’ embedding â†’ [shift, scale, gate] â†’ modulate features
Text handled separately via attention
```

### Time + Text Fused (SD3, FLUX, Hunyuan)

```python
timestep_emb + pooled_text_emb â†’ [shift, scale, gate]
Both global signals fused for modulation
Sequential text still in separate stream
```

### Time as Token (OmniGen)

```python
time_token = embed(timestep)
sequence = [text_tokens, time_token, image_tokens]
Process via attention (time is just another token)
```

---

## ðŸŽ“ For Your Audience

### **Your Figure is CORRECT!** âœ…

1. **PixArt/Lumina** shows **separate** text input â†’ Cross-attention âœ“
2. **SD3** shows **two streams** with âŠ™ symbol â†’ Double-stream âœ“
3. **FLUX** shows **mixed architecture** â†’ Hybrid approach âœ“
4. **OmniGen** shows **concatenation** âŠ™ â†’ Single-stream âœ“
5. **Timestep labels** correctly distinguish:
   - "Time" alone = separate modulation
   - "Time + Text" = fused conditioning
   - Time with âŠ™ = token concatenation

### **Key Takeaway**

> **There is NO conflict!** Different models use fundamentally different strategies:
>
> - **Cross-Attention**: Keep modalities SEPARATE
> - **Double-Stream**: Process in PARALLEL
> - **Single-Stream**: CONCATENATE everything
>
> Each approach has different trade-offs for efficiency, expressiveness, and simplicity.

---

## ðŸ“š Reference

See `comprehensive_dit_architecture_comparison.md` for:

- Detailed code references with line numbers
- Complete implementation details
- Performance implications
- Architecture decision tree

**Code Base**: HuggingFace Diffusers (validated against actual implementations)
